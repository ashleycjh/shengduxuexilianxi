{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 导入必要的库函数",
   "id": "82d8a5ea1c329333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T06:42:36.761565Z",
     "start_time": "2024-07-31T06:42:35.201347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "%matplotlib qt"
   ],
   "id": "88e966a81aec462b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T06:42:36.765756Z",
     "start_time": "2024-07-31T06:42:36.762081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_accuracies = {\n",
    "    'lennet_5': []\n",
    "}\n",
    "print(model_accuracies)"
   ],
   "id": "dd6a1e079fc59dcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lennet_5': []}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 深度学习模型一 lennet-5",
   "id": "ddc1272eb738b8fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.图像转化",
   "id": "4a5b352986bfdb01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T06:42:36.772455Z",
     "start_time": "2024-07-31T06:42:36.765756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class Cutout(object):\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)  # 转换为 NumPy 数组\n",
    "        if img.ndim == 2:  # 如果是灰度图像\n",
    "            img = np.expand_dims(img, axis=-1)  # 添加一个通道维度\n",
    "        h, w, c = img.shape\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for _ in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "            mask[y1:y2, x1:x2] = 0\n",
    "\n",
    "        img = img * mask[:, :, np.newaxis]\n",
    "\n",
    "        # 确保像素值在 [0, 255] 范围内，并转换为 uint8\n",
    "        img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # 处理灰度图像\n",
    "        if img.shape[2] == 1:\n",
    "            img = np.squeeze(img, axis=-1)  # 去掉通道维度\n",
    "\n",
    "        return Image.fromarray(img)\n",
    "\n",
    "\n",
    "\n",
    "# 定义批量大小\n",
    "batch_size = 100\n",
    "# 数据增强和预处理\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # 图像大小转化\n",
    "    #transforms.RandomRotation(degrees=(0, 180)),  # 旋转\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 平移\n",
    "    #transforms.RandomResizedCrop(size=(32, 32), scale=(0.8, 1.0)),  # 缩放\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),  # 水平翻转\n",
    "    #transforms.RandomAffine(degrees=0, shear=(-10, 10, -10, 10)),  # 剪切\n",
    "    #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  # 调整亮度、对比度、饱和度和色调\n",
    "    transforms.ToTensor(),  # 将图像转换为PyTorch的张量格式，并将像素值从0-255缩放到0-1之间\n",
    "    #AddGaussianNoise(std=0.1),  # 添加高斯噪声\n",
    "    #Cutout(n_holes=1, length=16),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 标准化图像，使每个通道的像素值分布在[-1, 1]之间\n",
    "])\n"
   ],
   "id": "6b3456549e097c8c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.载入图片数据",
   "id": "c4a99ea35ff224d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T06:42:36.787265Z",
     "start_time": "2024-07-31T06:42:36.773457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载数据集\n",
    "data_dir = r'D:\\深度学习练习\\flower_photos\\flower_photos'\n",
    "train_dataset = datasets.ImageFolder(root=data_dir, transform=transform1)#数据转化\n",
    "train_size = int(0.8 * len(train_dataset))#数据集划分，2：8划分，确定训练集大小\n",
    "val_size = len(train_dataset) - train_size#确定测试集大小\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])#随机从数据集中分配数据到测试，训练集\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)#数据分批载入训练集，每批次大小为batch_size,shuffle=True在每个epoch开始时，数据集中的样本会被随机重排\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "f8083409a3bb01c5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.定义模型1，lenet-5",
   "id": "6bbb63b894a21cdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T06:42:36.812581Z",
     "start_time": "2024-07-31T06:42:36.787265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义 LeNet-5 模型\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, padding=2)  # 3个通道输入输出6，第一层卷积窗口大小为5，步长默认为1\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)#第一层池化，使用平均池化，窗口大小为2，步长为2，把卷积后的图片放小为原理的四分之一\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 调整展平后的大小\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)  # 输出5类\n",
    "\n",
    "    def forward(self, x):#向前传播更新\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        #print(f\"out shape1: {x.shape}\")\n",
    "        x = self.pool1(x)\n",
    "        #print(f\"out shape1: {x.shape}\")\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        #print(f\"out shape2: {x.shape}\")\n",
    "        x = self.pool2(x)\n",
    "        #print(f\"out shape3: {x.shape}\")\n",
    "        x = x.view(-1, 16 * 6 * 6)  # 确保形状匹配\n",
    "        #print(f\"out shape4: {x.shape}\")\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        #print(f\"out shape5: {x.shape}\")\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        #print(f\"out shape6: {x.shape}\")\n",
    "        x = self.fc3(x)\n",
    "        #print(f\"out shape7: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "# 参数设置\n",
    "learning_rate = 0.0001#学习效率\n",
    "epochs = 40#训练次数\n",
    "# 检查GPU是否可用，并把模型部署到GPU上\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "23f7d7fad8ccd43e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.初始化训练模型1",
   "id": "493008550649885c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T06:45:44.494170Z",
     "start_time": "2024-07-31T06:42:36.812581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 初始化模型、损失函数和优化器\n",
    "model = LeNet5(num_classes=5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()#选择CrossEntropyLoss为损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)# 优化器选择为Adam\n",
    "#效果很差criterion = nn.NLLLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "accuracies = []\n",
    "# 训练模型\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()#将模型设置为训练模式\n",
    "    running_loss = 0.0# 初始化一个变量来累积每个epoch的损失\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)#将训练数据部署到GPU上\n",
    "        optimizer.zero_grad()# 清除优化器的梯度\n",
    "        outputs = model(inputs) # 前向传播：使用模型预测输出\n",
    "        loss = criterion(outputs, labels)#计算损失\n",
    "        loss.backward()# 反向传播：计算梯度\n",
    "        optimizer.step()#  # 更新模型参数\n",
    "        running_loss += loss.item()#  累积损失值\n",
    "    train_losses.append(running_loss / len(train_loader))# 计算平均损失并存储\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {running_loss / len(train_loader)}\")\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # 将数据移动到GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0) # 累积总的样本数\n",
    "            correct += (predicted == labels).sum().item() # 累积正确的样本数\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    # 将准确率存储到 model_accuracies 字典中\n",
    "    model_accuracies['lennet_5'].append(accuracy)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Validation Accuracy: {accuracy:.2f}%')\n",
    "# 计算验证集的准确率\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # 将数据移动到GPU\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n"
   ],
   "id": "e3e2f82f8f4a2d1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.604751189549764\n",
      "Epoch [1/40], Validation Accuracy: 30.25%\n",
      "Epoch 2, Train Loss: 1.5712368845939637\n",
      "Epoch [2/40], Validation Accuracy: 33.38%\n",
      "Epoch 3, Train Loss: 1.4854562997817993\n",
      "Epoch [3/40], Validation Accuracy: 44.01%\n",
      "Epoch 4, Train Loss: 1.3498377760251363\n",
      "Epoch [4/40], Validation Accuracy: 47.14%\n",
      "Epoch 5, Train Loss: 1.2428682684898376\n",
      "Epoch [5/40], Validation Accuracy: 50.14%\n",
      "Epoch 6, Train Loss: 1.2001262466112772\n",
      "Epoch [6/40], Validation Accuracy: 47.68%\n",
      "Epoch 7, Train Loss: 1.1899515827496847\n",
      "Epoch [7/40], Validation Accuracy: 50.00%\n",
      "Epoch 8, Train Loss: 1.1722808639208475\n",
      "Epoch [8/40], Validation Accuracy: 49.32%\n",
      "Epoch 9, Train Loss: 1.164195716381073\n",
      "Epoch [9/40], Validation Accuracy: 51.91%\n",
      "Epoch 10, Train Loss: 1.1614164153734843\n",
      "Epoch [10/40], Validation Accuracy: 50.54%\n",
      "Epoch 11, Train Loss: 1.1488386034965514\n",
      "Epoch [11/40], Validation Accuracy: 50.14%\n",
      "Epoch 12, Train Loss: 1.144299284617106\n",
      "Epoch [12/40], Validation Accuracy: 52.18%\n",
      "Epoch 13, Train Loss: 1.1313691476980845\n",
      "Epoch [13/40], Validation Accuracy: 51.23%\n",
      "Epoch 14, Train Loss: 1.1274586121241252\n",
      "Epoch [14/40], Validation Accuracy: 51.09%\n",
      "Epoch 15, Train Loss: 1.124545786778132\n",
      "Epoch [15/40], Validation Accuracy: 52.32%\n",
      "Epoch 16, Train Loss: 1.111775271097819\n",
      "Epoch [16/40], Validation Accuracy: 52.18%\n",
      "Epoch 17, Train Loss: 1.1068587601184845\n",
      "Epoch [17/40], Validation Accuracy: 51.63%\n",
      "Epoch 18, Train Loss: 1.1033713479836782\n",
      "Epoch [18/40], Validation Accuracy: 51.36%\n",
      "Epoch 19, Train Loss: 1.0900078097979227\n",
      "Epoch [19/40], Validation Accuracy: 51.63%\n",
      "Epoch 20, Train Loss: 1.085256580511729\n",
      "Epoch [20/40], Validation Accuracy: 52.45%\n",
      "Epoch 21, Train Loss: 1.0767447392145792\n",
      "Epoch [21/40], Validation Accuracy: 52.04%\n",
      "Epoch 22, Train Loss: 1.0732160945733389\n",
      "Epoch [22/40], Validation Accuracy: 52.32%\n",
      "Epoch 23, Train Loss: 1.0667743881543477\n",
      "Epoch [23/40], Validation Accuracy: 50.95%\n",
      "Epoch 24, Train Loss: 1.0581106861432394\n",
      "Epoch [24/40], Validation Accuracy: 52.45%\n",
      "Epoch 25, Train Loss: 1.054628445704778\n",
      "Epoch [25/40], Validation Accuracy: 52.04%\n",
      "Epoch 26, Train Loss: 1.0472046971321105\n",
      "Epoch [26/40], Validation Accuracy: 52.72%\n",
      "Epoch 27, Train Loss: 1.0403351366519928\n",
      "Epoch [27/40], Validation Accuracy: 51.91%\n",
      "Epoch 28, Train Loss: 1.035156911611557\n",
      "Epoch [28/40], Validation Accuracy: 52.86%\n",
      "Epoch 29, Train Loss: 1.0245319585005441\n",
      "Epoch [29/40], Validation Accuracy: 50.68%\n",
      "Epoch 30, Train Loss: 1.0189904888470969\n",
      "Epoch [30/40], Validation Accuracy: 50.54%\n",
      "Epoch 31, Train Loss: 1.01564968029658\n",
      "Epoch [31/40], Validation Accuracy: 50.68%\n",
      "Epoch 32, Train Loss: 1.007831758260727\n",
      "Epoch [32/40], Validation Accuracy: 52.72%\n",
      "Epoch 33, Train Loss: 1.0011127293109894\n",
      "Epoch [33/40], Validation Accuracy: 52.32%\n",
      "Epoch 34, Train Loss: 0.9997203946113586\n",
      "Epoch [34/40], Validation Accuracy: 50.82%\n",
      "Epoch 35, Train Loss: 0.9912049392859141\n",
      "Epoch [35/40], Validation Accuracy: 52.18%\n",
      "Epoch 36, Train Loss: 0.9871796568234762\n",
      "Epoch [36/40], Validation Accuracy: 52.86%\n",
      "Epoch 37, Train Loss: 0.9774754583835602\n",
      "Epoch [37/40], Validation Accuracy: 51.36%\n",
      "Epoch 38, Train Loss: 0.9700105826059977\n",
      "Epoch [38/40], Validation Accuracy: 52.32%\n",
      "Epoch 39, Train Loss: 0.9627134323120117\n",
      "Epoch [39/40], Validation Accuracy: 52.04%\n",
      "Epoch 40, Train Loss: 0.9618863642215729\n",
      "Epoch [40/40], Validation Accuracy: 51.36%\n",
      "Validation Accuracy: 51.36%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.保存模型",
   "id": "8606b87081367746"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T06:45:44.721774Z",
     "start_time": "2024-07-31T06:45:44.495211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存模型权重\n",
    "torch.save(model.state_dict(), 'lenet5_weights3.pth')\n",
    "# 保存模型结构和权重\n",
    "torch.save(model, 'lenet5_model3.pth')\n",
    "accuracies_list = model_accuracies['lennet_5']\n",
    "\n",
    "# 创建一个包含epoch编号和对应准确率的字典\n",
    "data = {'Epoch': list(range(1, len(accuracies_list) + 1)),\n",
    "        'Accuracy': accuracies_list}\n",
    "\n",
    "# 将字典转换为DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 现在你可以将DataFrame保存为Excel文件\n",
    "df.to_excel('lennet_5_accuracies3.xlsx', index=False)\n",
    "# 清理内存\n",
    "del model\n",
    "del inputs\n",
    "del labels\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "ad66c03127741fd7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### （示例）下次调用时可加载模型",
   "id": "c23f3f7baebe4fd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T06:45:44.739206Z",
     "start_time": "2024-07-31T06:45:44.721774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# 实例化模型\n",
    "model = LeNet5(num_classes=5).to(device)\n",
    "\n",
    "# 加载模型权重\n",
    "model.load_state_dict(torch.load('lenet5_weights3.pth'))\n",
    "\n",
    "# 设置为评估模式\n",
    "model.eval()"
   ],
   "id": "cd7a942699724a4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_16396\\2478517745.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('lenet5_weights3.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ## 评估模型",
   "id": "e499185fd518b79f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T06:45:45.691131Z",
     "start_time": "2024-07-31T06:45:44.739206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 确保模型处于评估模式\n",
    "model.eval()\n",
    "\n",
    "# 初始化用于存储预测和真实标签的列表\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# 不计算梯度进行预测\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        # 将数据移动到GPU，如果可用\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 获取预测概率最高的类别\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # 将预测结果和真实标签添加到列表中\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 计算准确率、精确率、召回率和F1分数\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "\n",
    "# 将结果保存到CSV文件\n",
    "results = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "}\n",
    "\n",
    "# 写入CSV文件\n",
    "with open('lennet5评估3.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Value'])\n",
    "    for metric, value in results.items():\n",
    "        writer.writerow([metric, value])\n",
    "\n",
    "print(\"Results have been saved to lennet5评估3.csv\")"
   ],
   "id": "e233cde0f15dc8ce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 8/8 [00:00<00:00,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to lennet5评估3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T06:45:45.698478Z",
     "start_time": "2024-07-31T06:45:45.696100Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ab8c85f96ac555e1",
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
