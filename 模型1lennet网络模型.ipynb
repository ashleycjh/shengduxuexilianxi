{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 导入必要的库函数",
   "id": "82d8a5ea1c329333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T05:56:03.313414Z",
     "start_time": "2024-07-31T05:55:59.496454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "%matplotlib qt"
   ],
   "id": "88e966a81aec462b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T05:56:08.460580Z",
     "start_time": "2024-07-31T05:56:08.457186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_accuracies = {\n",
    "    'lennet_5': []\n",
    "}\n",
    "print(model_accuracies)"
   ],
   "id": "dd6a1e079fc59dcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lennet_5': []}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 深度学习模型一 lennet-5",
   "id": "ddc1272eb738b8fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.图像转化",
   "id": "4a5b352986bfdb01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T05:56:09.994182Z",
     "start_time": "2024-07-31T05:56:09.989067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class Cutout(object):\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)  # 转换为 NumPy 数组\n",
    "        if img.ndim == 2:  # 如果是灰度图像\n",
    "            img = np.expand_dims(img, axis=-1)  # 添加一个通道维度\n",
    "        h, w, c = img.shape\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for _ in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "            mask[y1:y2, x1:x2] = 0\n",
    "\n",
    "        img = img * mask[:, :, np.newaxis]\n",
    "\n",
    "        # 确保像素值在 [0, 255] 范围内，并转换为 uint8\n",
    "        img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # 处理灰度图像\n",
    "        if img.shape[2] == 1:\n",
    "            img = np.squeeze(img, axis=-1)  # 去掉通道维度\n",
    "\n",
    "        return Image.fromarray(img)\n",
    "\n",
    "\n",
    "\n",
    "# 定义批量大小\n",
    "batch_size = 100\n",
    "# 数据增强和预处理\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # 图像大小转化\n",
    "    #transforms.RandomRotation(degrees=(0, 180)),  # 旋转\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 平移\n",
    "    #transforms.RandomResizedCrop(size=(32, 32), scale=(0.8, 1.0)),  # 缩放\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),  # 水平翻转\n",
    "    #transforms.RandomAffine(degrees=0, shear=(-10, 10, -10, 10)),  # 剪切\n",
    "    #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  # 调整亮度、对比度、饱和度和色调\n",
    "    transforms.ToTensor(),  # 将图像转换为PyTorch的张量格式，并将像素值从0-255缩放到0-1之间\n",
    "    #AddGaussianNoise(std=0.1),  # 添加高斯噪声\n",
    "    #Cutout(n_holes=1, length=16),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 标准化图像，使每个通道的像素值分布在[-1, 1]之间\n",
    "])\n"
   ],
   "id": "6b3456549e097c8c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.载入图片数据",
   "id": "c4a99ea35ff224d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T05:56:11.504411Z",
     "start_time": "2024-07-31T05:56:11.486901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载数据集\n",
    "data_dir = r'D:\\深度学习练习\\flower_photos\\flower_photos'\n",
    "train_dataset = datasets.ImageFolder(root=data_dir, transform=transform1)#数据转化\n",
    "train_size = int(0.8 * len(train_dataset))#数据集划分，2：8划分，确定训练集大小\n",
    "val_size = len(train_dataset) - train_size#确定测试集大小\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])#随机从数据集中分配数据到测试，训练集\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)#数据分批载入训练集，每批次大小为batch_size,shuffle=True在每个epoch开始时，数据集中的样本会被随机重排\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "f8083409a3bb01c5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.定义模型1，lenet-5",
   "id": "6bbb63b894a21cdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T05:56:12.667615Z",
     "start_time": "2024-07-31T05:56:12.636898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义 LeNet-5 模型\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, padding=2)  # 3个通道输入输出6，第一层卷积窗口大小为5，步长默认为1\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)#第一层池化，使用平均池化，窗口大小为2，步长为2，把卷积后的图片放小为原理的四分之一\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 调整展平后的大小\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)  # 输出5类\n",
    "\n",
    "    def forward(self, x):#向前传播更新\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        print(f\"out shape1: {x.shape}\")\n",
    "        x = self.pool1(x)\n",
    "        print(f\"out shape1: {x.shape}\")\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        print(f\"out shape2: {x.shape}\")\n",
    "        x = self.pool2(x)\n",
    "        print(f\"out shape3: {x.shape}\")\n",
    "        x = x.view(-1, 16 * 6 * 6)  # 确保形状匹配\n",
    "        print(f\"out shape4: {x.shape}\")\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        print(f\"out shape5: {x.shape}\")\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        print(f\"out shape6: {x.shape}\")\n",
    "        x = self.fc3(x)\n",
    "        print(f\"out shape7: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "# 参数设置\n",
    "learning_rate = 0.0001#学习效率\n",
    "epochs = 40#训练次数\n",
    "# 检查GPU是否可用，并把模型部署到GPU上\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "23f7d7fad8ccd43e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.初始化训练模型1",
   "id": "493008550649885c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T05:56:18.490416Z",
     "start_time": "2024-07-31T05:56:14.166719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 初始化模型、损失函数和优化器\n",
    "model = LeNet5(num_classes=5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()#选择CrossEntropyLoss为损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)# 优化器选择为Adam\n",
    "#效果很差criterion = nn.NLLLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "accuracies = []\n",
    "# 训练模型\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()#将模型设置为训练模式\n",
    "    running_loss = 0.0# 初始化一个变量来累积每个epoch的损失\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)#将训练数据部署到GPU上\n",
    "        optimizer.zero_grad()# 清除优化器的梯度\n",
    "        outputs = model(inputs) # 前向传播：使用模型预测输出\n",
    "        loss = criterion(outputs, labels)#计算损失\n",
    "        loss.backward()# 反向传播：计算梯度\n",
    "        optimizer.step()#  # 更新模型参数\n",
    "        running_loss += loss.item()#  累积损失值\n",
    "    train_losses.append(running_loss / len(train_loader))# 计算平均损失并存储\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {running_loss / len(train_loader)}\")\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # 将数据移动到GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0) # 累积总的样本数\n",
    "            correct += (predicted == labels).sum().item() # 累积正确的样本数\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    # 将准确率存储到 model_accuracies 字典中\n",
    "    model_accuracies['lennet_5'].append(accuracy)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Validation Accuracy: {accuracy:.2f}%')\n",
    "# 计算验证集的准确率\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # 将数据移动到GPU\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n"
   ],
   "id": "e3e2f82f8f4a2d1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n",
      "out shape1: torch.Size([100, 6, 32, 32])\n",
      "out shape1: torch.Size([100, 6, 16, 16])\n",
      "out shape2: torch.Size([100, 16, 12, 12])\n",
      "out shape3: torch.Size([100, 16, 6, 6])\n",
      "out shape4: torch.Size([100, 576])\n",
      "out shape5: torch.Size([100, 120])\n",
      "out shape6: torch.Size([100, 84])\n",
      "out shape7: torch.Size([100, 5])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\u001B[38;5;66;03m#将模型设置为训练模式\u001B[39;00m\n\u001B[0;32m     14\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\u001B[38;5;66;03m# 初始化一个变量来累积每个epoch的损失\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     16\u001B[0m     inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;66;03m#将训练数据部署到GPU上\u001B[39;00m\n\u001B[0;32m     17\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\u001B[38;5;66;03m# 清除优化器的梯度\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[1;32m---> 50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[1;34m(self, indices)\u001B[0m\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    245\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader(path)\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 247\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(sample)\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    249\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m t(img)\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001B[0m, in \u001B[0;36mResize.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m    347\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    349\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 354\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mresize(img, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minterpolation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mantialias)\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[0;32m    475\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    476\u001B[0m     pil_interpolation \u001B[38;5;241m=\u001B[39m pil_modes_mapping[interpolation]\n\u001B[1;32m--> 477\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F_pil\u001B[38;5;241m.\u001B[39mresize(img, size\u001B[38;5;241m=\u001B[39moutput_size, interpolation\u001B[38;5;241m=\u001B[39mpil_interpolation)\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mresize(img, size\u001B[38;5;241m=\u001B[39moutput_size, interpolation\u001B[38;5;241m=\u001B[39minterpolation\u001B[38;5;241m.\u001B[39mvalue, antialias\u001B[38;5;241m=\u001B[39mantialias)\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(size, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot inappropriate size arg: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 250\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\u001B[38;5;241m.\u001B[39mresize(\u001B[38;5;28mtuple\u001B[39m(size[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), interpolation)\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\PIL\\Image.py:2222\u001B[0m, in \u001B[0;36mImage.resize\u001B[1;34m(self, size, resample, box, reducing_gap)\u001B[0m\n\u001B[0;32m   2210\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2211\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduce(factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[0;32m   2212\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduce)\n\u001B[0;32m   2213\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m Image\u001B[38;5;241m.\u001B[39mreduce(\u001B[38;5;28mself\u001B[39m, factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[0;32m   2214\u001B[0m         )\n\u001B[0;32m   2215\u001B[0m         box \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2216\u001B[0m             (box[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[0;32m   2217\u001B[0m             (box[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[0;32m   2218\u001B[0m             (box[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[0;32m   2219\u001B[0m             (box[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[0;32m   2220\u001B[0m         )\n\u001B[1;32m-> 2222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mim\u001B[38;5;241m.\u001B[39mresize(size, resample, box))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.保存模型",
   "id": "8606b87081367746"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 保存模型权重\n",
    "torch.save(model.state_dict(), 'lenet5_weights3.pth')\n",
    "# 保存模型结构和权重\n",
    "torch.save(model, 'lenet5_model3.pth')\n",
    "accuracies_list = model_accuracies['lennet_5']\n",
    "\n",
    "# 创建一个包含epoch编号和对应准确率的字典\n",
    "data = {'Epoch': list(range(1, len(accuracies_list) + 1)),\n",
    "        'Accuracy': accuracies_list}\n",
    "\n",
    "# 将字典转换为DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 现在你可以将DataFrame保存为Excel文件\n",
    "df.to_excel('lennet_5_accuracies3.xlsx', index=False)\n",
    "# 清理内存\n",
    "del model\n",
    "del inputs\n",
    "del labels\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "ad66c03127741fd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### （示例）下次调用时可加载模型",
   "id": "c23f3f7baebe4fd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# 实例化模型\n",
    "model = LeNet5(num_classes=5).to(device)\n",
    "\n",
    "# 加载模型权重\n",
    "model.load_state_dict(torch.load('lenet5_weights3.pth'))\n",
    "\n",
    "# 设置为评估模式\n",
    "model.eval()"
   ],
   "id": "cd7a942699724a4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ## 评估模型",
   "id": "e499185fd518b79f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 确保模型处于评估模式\n",
    "model.eval()\n",
    "\n",
    "# 初始化用于存储预测和真实标签的列表\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# 不计算梯度进行预测\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        # 将数据移动到GPU，如果可用\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 获取预测概率最高的类别\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # 将预测结果和真实标签添加到列表中\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 计算准确率、精确率、召回率和F1分数\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "\n",
    "# 将结果保存到CSV文件\n",
    "results = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "}\n",
    "\n",
    "# 写入CSV文件\n",
    "with open('lennet5评估3.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Value'])\n",
    "    for metric, value in results.items():\n",
    "        writer.writerow([metric, value])\n",
    "\n",
    "print(\"Results have been saved to lennet5评估3.csv\")"
   ],
   "id": "e233cde0f15dc8ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "# 加载图像\n",
    "img = Image.open(r'D:\\深度学习练习\\Data\\1_ tg(1).jpg')\n",
    "\n",
    "# 获取图像模式\n",
    "mode = img.mode\n",
    "\n",
    "# 检查图像模式\n",
    "if mode == 'L':\n",
    "    print(\"This is a grayscale image.\")\n",
    "elif mode == 'RGB':\n",
    "    print(\"This is an RGB color image.\")\n",
    "else:\n",
    "    print(\"This is an image with another mode.\")"
   ],
   "id": "86969d0e9bb6f227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ab8c85f96ac555e1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
